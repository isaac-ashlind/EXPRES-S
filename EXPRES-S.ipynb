{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e3fad2",
   "metadata": {},
   "source": [
    "## import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb1a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as db\n",
    "from scipy.integrate import trapezoid\n",
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from astropy.nddata import StdDevUncertainty\n",
    "from PyAstronomy import pyasl\n",
    "from specutils import Spectrum1D\n",
    "from specutils.manipulation import gaussian_smooth, SplineInterpolatedResampler\n",
    "from intersect import intersection\n",
    "from itertools import chain\n",
    "from IPython.display import clear_output\n",
    "from pqdm.processes import pqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1800635",
   "metadata": {},
   "source": [
    "## global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56db2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', np.RankWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545beb8",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5e6bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent directory containing FITS file folders\n",
    "parent = \"/media/solar_data_new/extracted/fitspec/2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f46ff7",
   "metadata": {},
   "source": [
    "## get filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05c6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = [x for x in os.listdir(parent) if x.endswith(\"solar\")]\n",
    "folder_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565c2f6",
   "metadata": {},
   "source": [
    "## get doppler shift offsets & quality factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34e792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(\"postgresql://solar:lowell@10.10.115.133/solar\")\n",
    "bc = pd.read_sql('''SELECT * FROM bc''', engine)\n",
    "vels = pd.read_sql(\"SELECT * FROM velocity\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c40a4",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2d1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# echelle spectra orders of interest\n",
    "first = 3\n",
    "last = 8\n",
    "\n",
    "# wavelengths used for resampling (Angstroms)\n",
    "start = 3890\n",
    "stop = 4020\n",
    "step = 0.01\n",
    "\n",
    "# polynomial degree for fitting error\n",
    "polyfit_deg = 4\n",
    "\n",
    "# width of Gaussian for smoothing (standard deviations)\n",
    "gauss_width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f46550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build wave grid for spline resampling\n",
    "orders = list(range(first, last))\n",
    "wave_grid = np.arange(start, stop, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffca1e2",
   "metadata": {},
   "source": [
    "## build filters (based on MWO HPK-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69724ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust filter midpoints from air to vacuum values\n",
    "midpts_air = [3901.07, 3933.68, 3968.49, 4001.07]\n",
    "midpts_vac = pyasl.airtovac2(midpts_air)\n",
    "\n",
    "# get filter midpoints and widths\n",
    "V_mid = midpts_vac[0]\n",
    "K_mid = midpts_vac[1]\n",
    "H_mid = midpts_vac[2]\n",
    "R_mid = midpts_vac[3]\n",
    "FWHM = 1.09 # full width at half max for H & K filters\n",
    "BPHW = 10   # band pass half width for R & V pseudo-continuum\n",
    "\n",
    "# rectangular filter\n",
    "def R_pass(wavelength):\n",
    "    if (wavelength > R_mid - BPHW) and (wavelength < R_mid + BPHW):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# rectangular filter\n",
    "def V_pass(wavelength):\n",
    "    if (wavelength > V_mid - BPHW) and (wavelength < V_mid + BPHW):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# triangular filter\n",
    "def H_pass(wavelength):\n",
    "    if (wavelength > H_mid - FWHM) and (wavelength < H_mid + FWHM):\n",
    "        slope = 1/FWHM\n",
    "        x = np.abs(H_mid - wavelength)\n",
    "        return -x*slope + 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# triangular filter\n",
    "def K_pass(wavelength):\n",
    "    if (wavelength > K_mid - FWHM) and (wavelength < K_mid + FWHM):\n",
    "        slope = 1/FWHM\n",
    "        x = np.abs(K_mid - wavelength)\n",
    "        return -x*slope + 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "R_filter = [R_pass(x) for x in wave_grid]\n",
    "V_filter = [V_pass(x) for x in wave_grid]\n",
    "H_filter = [H_pass(x) for x in wave_grid]\n",
    "K_filter = [K_pass(x) for x in wave_grid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02691fed",
   "metadata": {},
   "source": [
    "\n",
    "## take filepath, return s-index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89aa9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expres_s(directory):\n",
    "    \"\"\"\n",
    "    building this into a function allows for parallel processing w/ pqdm\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract date\n",
    "    date = os.path.basename(directory)[:6]\n",
    "    \n",
    "    # get fits files\n",
    "    file_list = os.listdir(directory)\n",
    "    file_list = [i for i in file_list if i.startswith(\"Sun\")]\n",
    "\n",
    "    # build DataFrames\n",
    "    spec_df = pd.DataFrame(index=file_list, columns=wave_grid, dtype=\"float64\")\n",
    "    err_df = pd.DataFrame(index=file_list, columns=wave_grid, dtype=\"float64\")\n",
    "\n",
    "    # keep track of bad fits files\n",
    "    no_quality = []\n",
    "    lo_quality = []\n",
    "    error_fits = []\n",
    "\n",
    "    for file in file_list:\n",
    "        with fits.open(f\"{directory}/{file}\") as hdu:\n",
    "            # get observation number for quality factor & doppler shift\n",
    "            obnm = os.path.basename(file).split('Sun_')[1][0:-5]\n",
    "\n",
    "            # filter spectra based on quality factor\n",
    "            quality = vels.query(f\"obnm == {obnm}\")[\"quality\"]\n",
    "\n",
    "            # check for no quality factor\n",
    "            if quality.empty:\n",
    "                no_quality.append(file)\n",
    "                continue\n",
    "\n",
    "            # check for low quality factor\n",
    "            if quality.iloc[0] < 0.95:\n",
    "                lo_quality.append(file)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # get data from fits file\n",
    "                w_nans = list(hdu[1].data[\"wavelength\"][orders])\n",
    "                s_nans = list(hdu[1].data[\"spectrum\"][orders])\n",
    "                c_nans = list(hdu[1].data[\"continuum\"][orders])\n",
    "                e_nans = list(hdu[1].data[\"uncertainty\"][orders])\n",
    "\n",
    "                w = []\n",
    "                s = []\n",
    "                c = []\n",
    "                e = []\n",
    "\n",
    "                # mask nans\n",
    "                for i in range(len(orders)):\n",
    "                    nan_mask = ~np.isnan(s_nans)[i]\n",
    "                    w.append(w_nans[i][nan_mask])\n",
    "                    s.append(s_nans[i][nan_mask])\n",
    "                    c.append(c_nans[i][nan_mask])\n",
    "                    e.append(e_nans[i][nan_mask])\n",
    "\n",
    "                # find error polyfit intersections and truncate orders\n",
    "                for i in range(len(orders) - 1):\n",
    "\n",
    "                    # get order overlap interval\n",
    "                    a_last = w[i][-1]\n",
    "                    b_first = w[i+1][0]\n",
    "                    interval = np.arange(b_first, a_last, 0.01)\n",
    "\n",
    "                    # fit polynomial to adjacent orders' error\n",
    "                    degree = 4\n",
    "                    err_fit1 = np.polyfit(w[i], e[i], deg=polyfit_deg)\n",
    "                    err_fit2 = np.polyfit(w[i+1], e[i+1], deg=polyfit_deg)\n",
    "\n",
    "                    # get error polyfit intersection\n",
    "                    x1 = interval\n",
    "                    y1 = np.poly1d(err_fit1)(interval)\n",
    "                    x2 = interval\n",
    "                    y2 = np.poly1d(err_fit2)(interval)\n",
    "                    x, y = intersection(x1, y1, x2, y2)\n",
    "\n",
    "                    # truncate based on intersection\n",
    "                    cut = float(x.mean())\n",
    "                    a_mask = [x < cut for x in w[i]]\n",
    "                    b_mask = [x > cut for x in w[i+1]]\n",
    "\n",
    "                    w[i] = w[i][a_mask]\n",
    "                    s[i] = s[i][a_mask]\n",
    "                    c[i] = c[i][a_mask]\n",
    "                    e[i] = e[i][a_mask]\n",
    "\n",
    "                    w[i+1] = w[i+1][b_mask]\n",
    "                    s[i+1] = s[i+1][b_mask]\n",
    "                    c[i+1] = c[i+1][b_mask]\n",
    "                    e[i+1] = e[i+1][b_mask]\n",
    "\n",
    "                # flatten spectrum\n",
    "                w_flat = np.array(list(chain.from_iterable(w)))\n",
    "                s_flat = np.array(list(chain.from_iterable(s)))\n",
    "                c_flat = np.array(list(chain.from_iterable(c)))\n",
    "                e_flat = np.array(list(chain.from_iterable(e)))\n",
    "\n",
    "                # normalize flux with continuum\n",
    "                s_norm = np.divide(s_flat, c_flat)\n",
    "\n",
    "                # get doppler shift velocity and convert to km/s\n",
    "                shift = int(bc.query(f\"obnm == {obnm}\")[\"bc\"])/10e3\n",
    "\n",
    "                # doppler shift\n",
    "                s_shifted, w_shifted = pyasl.dopplerShift(w_flat, s_norm, shift)\n",
    "                e_shifted, w_shifted = pyasl.dopplerShift(w_flat, e_flat, shift)\n",
    "                \n",
    "                # create Spectrum1D object and smooth w/ Gaussian\n",
    "                spec_object = Spectrum1D(spectral_axis=w_shifted*u.Angstrom, \n",
    "                                         flux=s_shifted*u.dimensionless_unscaled, \n",
    "                                         uncertainty=StdDevUncertainty(e_shifted*u.dimensionless_unscaled))\n",
    "                spec_smooth = gaussian_smooth(spec_object, stddev=gauss_width)\n",
    "\n",
    "                # resample with spline over activity band\n",
    "                spline = SplineInterpolatedResampler()\n",
    "                spec_spline = spline(spec_smooth, wave_grid*u.Angstrom)\n",
    "\n",
    "                # add spectrum and error to DataFrame objects\n",
    "                spec_df.loc[file] = spec_spline.flux\n",
    "                err_df.loc[file] = spec_spline.uncertainty.array\n",
    "\n",
    "            except:\n",
    "                error_fits.append(file)\n",
    "                continue\n",
    "\n",
    "    # drop bad files\n",
    "    bad_files = no_quality + lo_quality + error_fits\n",
    "    spec_df.drop(bad_files, inplace=True)\n",
    "    err_df.drop(bad_files, inplace=True)\n",
    "    \n",
    "    # return nan if too few observations\n",
    "    if len(spec_df.index) < 10:\n",
    "        return np.nan, date\n",
    "    \n",
    "    # build composite with spectra weighted by variance\n",
    "    variance = np.square(np.divide(1, err_df))\n",
    "    weighted = np.multiply(spec_df, variance)\n",
    "    var_sum = variance.sum(axis=0)\n",
    "    weight_sum = weighted.sum(axis=0)\n",
    "    expres = np.divide(weight_sum, var_sum)\n",
    "    \n",
    "    # Integrate spectra convolved w/ filters\n",
    "    R_flux = trapezoid(x=wave_grid, y=expres*R_filter)\n",
    "    V_flux = trapezoid(x=wave_grid, y=expres*V_filter)\n",
    "    H_flux = trapezoid(x=wave_grid, y=expres*H_filter)\n",
    "    K_flux = trapezoid(x=wave_grid, y=expres*K_filter)\n",
    "    \n",
    "    # calculate S!\n",
    "    # from Duncan 1991\n",
    "    # relative duty-cycle between line-core and continuum bandpasses\n",
    "    # MWO HPK-2 spent 80% of integration time on HK vs. 10% on RV\n",
    "    alpha = 2.4\n",
    "    HK = (H_flux + K_flux)*8 \n",
    "    RV = R_flux + V_flux     \n",
    "    s_value = alpha*HK/RV\n",
    "    \n",
    "    return s_value, date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd859af",
   "metadata": {},
   "source": [
    "## parallelized batch EXPRES-S calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68157967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829a7ffa0f984a978f02bbf46197fb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SUBMITTING | :   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1283ee6c82b8494eb63c1fcd25fa16a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING | :   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362fd29d15af4126a29e0f9d582c1716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING | :   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build iterable for pqdm\n",
    "directories = []\n",
    "for folder in folder_list:\n",
    "    directory = parent + '/' + folder\n",
    "    directories.append(directory)\n",
    "\n",
    "# get expres_s values\n",
    "result = pqdm(directories[:], get_expres_s, n_jobs=10)\n",
    "\n",
    "# cast results to array\n",
    "result_arr = np.array(result, dtype=\"object\")\n",
    "s_values = result_arr[:, 0].astype(\"float\")\n",
    "dates = result_arr[:, 1].astype(\"int\")\n",
    "\n",
    "# organize as Series and export pickle\n",
    "expres_s = pd.Series(s_values, index=dates)\n",
    "expres_s.dropna(inplace=True)\n",
    "expres_s.to_pickle(f\"Data/expres_s_{os.path.basename(parent)}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fde2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
